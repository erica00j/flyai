{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "954ea2b0-7c3d-430c-b49d-8e5ac3135a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정유리\n"
     ]
    }
   ],
   "source": [
    "print(\"정유리\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b467764-201a-4917-b59c-b51253f5e029",
   "metadata": {},
   "source": [
    "### 로또 당점 번호 수집"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d02afeaf-8029-4ac6-b67e-ce2d999cb57f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\2022-pc(t)-41\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\2022-pc(t)-41\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\2022-pc(t)-41\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\2022-pc(t)-41\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\2022-pc(t)-41\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests) (2023.11.17)\n",
      "Requirement already satisfied: BeautifulSoup4 in c:\\users\\2022-pc(t)-41\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.12.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\2022-pc(t)-41\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from BeautifulSoup4) (2.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install requests\n",
    "!pip install BeautifulSoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03e27591-9226-4482-a1b0-2dfb1126c226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12번 16번 21번 24번 41번 43번 15번 "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "res = requests.get(\"https://dhlottery.co.kr/common.do?method=main\")\n",
    "soup = BeautifulSoup(res.content,'html.parser')\n",
    "result = soup.select('.ball_645')\n",
    "for num in result:\n",
    "    print(num.text,end=\"번 \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64830398-06c4-4299-9104-60114f7c7733",
   "metadata": {},
   "source": [
    "### 네이버 뉴스 수집"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17edbf89-c203-4460-8c9f-6da8cb7801ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting newspaper3k\n",
      "  Downloading newspaper3k-0.2.8-py3-none-any.whl (211 kB)\n",
      "     ---------------------------------------- 0.0/211.1 kB ? eta -:--:--\n",
      "     - -------------------------------------- 10.2/211.1 kB ? eta -:--:--\n",
      "     ----- ------------------------------- 30.7/211.1 kB 660.6 kB/s eta 0:00:01\n",
      "     ----- ------------------------------- 30.7/211.1 kB 660.6 kB/s eta 0:00:01\n",
      "     ----- ------------------------------- 30.7/211.1 kB 660.6 kB/s eta 0:00:01\n",
      "     -------------- ---------------------- 81.9/211.1 kB 383.3 kB/s eta 0:00:01\n",
      "     ------------------- ---------------- 112.6/211.1 kB 437.6 kB/s eta 0:00:01\n",
      "     ------------------------------------ 211.1/211.1 kB 676.4 kB/s eta 0:00:00\n",
      "Requirement already satisfied: beautifulsoup4>=4.4.1 in c:\\users\\2022-pc(t)-41\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from newspaper3k) (4.12.2)\n",
      "Collecting Pillow>=3.3.0 (from newspaper3k)\n",
      "  Downloading Pillow-10.1.0-cp312-cp312-win_amd64.whl.metadata (9.6 kB)\n",
      "Requirement already satisfied: PyYAML>=3.11 in c:\\users\\2022-pc(t)-41\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from newspaper3k) (6.0.1)\n",
      "Collecting cssselect>=0.9.2 (from newspaper3k)\n",
      "  Downloading cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\n",
      "Collecting lxml>=3.6.0 (from newspaper3k)\n",
      "  Downloading lxml-4.9.3-cp312-cp312-win_amd64.whl.metadata (3.9 kB)\n",
      "Collecting nltk>=3.2.1 (from newspaper3k)\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "     ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "     --- ------------------------------------ 0.1/1.5 MB 4.3 MB/s eta 0:00:01\n",
      "     -------- ------------------------------- 0.3/1.5 MB 4.2 MB/s eta 0:00:01\n",
      "     -------------- ------------------------- 0.5/1.5 MB 4.3 MB/s eta 0:00:01\n",
      "     --------------------- ------------------ 0.8/1.5 MB 4.6 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 1.1/1.5 MB 4.6 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 1.4/1.5 MB 5.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.5/1.5 MB 5.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests>=2.10.0 in c:\\users\\2022-pc(t)-41\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from newspaper3k) (2.31.0)\n",
      "Collecting feedparser>=5.2.1 (from newspaper3k)\n",
      "  Downloading feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting tldextract>=2.0.1 (from newspaper3k)\n",
      "  Downloading tldextract-5.1.1-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting feedfinder2>=0.0.4 (from newspaper3k)\n",
      "  Downloading feedfinder2-0.0.4.tar.gz (3.3 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting jieba3k>=0.35.1 (from newspaper3k)\n",
      "  Downloading jieba3k-0.35.1.zip (7.4 MB)\n",
      "     ---------------------------------------- 0.0/7.4 MB ? eta -:--:--\n",
      "     - -------------------------------------- 0.4/7.4 MB 7.4 MB/s eta 0:00:01\n",
      "     --- ------------------------------------ 0.6/7.4 MB 8.0 MB/s eta 0:00:01\n",
      "     ----- ---------------------------------- 1.0/7.4 MB 7.4 MB/s eta 0:00:01\n",
      "     -------- ------------------------------- 1.5/7.4 MB 7.9 MB/s eta 0:00:01\n",
      "     ---------- ----------------------------- 1.9/7.4 MB 8.1 MB/s eta 0:00:01\n",
      "     ------------- -------------------------- 2.4/7.4 MB 8.6 MB/s eta 0:00:01\n",
      "     --------------- ------------------------ 2.8/7.4 MB 8.6 MB/s eta 0:00:01\n",
      "     ----------------- ---------------------- 3.3/7.4 MB 8.9 MB/s eta 0:00:01\n",
      "     ------------------- -------------------- 3.6/7.4 MB 8.2 MB/s eta 0:00:01\n",
      "     --------------------- ------------------ 3.9/7.4 MB 8.1 MB/s eta 0:00:01\n",
      "     ----------------------- ---------------- 4.4/7.4 MB 8.3 MB/s eta 0:00:01\n",
      "     -------------------------- ------------- 5.0/7.4 MB 8.6 MB/s eta 0:00:01\n",
      "     ---------------------------- ----------- 5.4/7.4 MB 8.6 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 5.7/7.4 MB 8.6 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 5.7/7.4 MB 8.2 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 6.4/7.4 MB 8.5 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 6.8/7.4 MB 8.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  7.3/7.4 MB 8.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 7.4/7.4 MB 8.3 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\2022-pc(t)-41\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from newspaper3k) (2.8.2)\n",
      "Collecting tinysegmenter==0.3 (from newspaper3k)\n",
      "  Downloading tinysegmenter-0.3.tar.gz (16 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\2022-pc(t)-41\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from beautifulsoup4>=4.4.1->newspaper3k) (2.5)\n",
      "Requirement already satisfied: six in c:\\users\\2022-pc(t)-41\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from feedfinder2>=0.0.4->newspaper3k) (1.16.0)\n",
      "Collecting sgmllib3k (from feedparser>=5.2.1->newspaper3k)\n",
      "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting click (from nltk>=3.2.1->newspaper3k)\n",
      "  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting joblib (from nltk>=3.2.1->newspaper3k)\n",
      "  Downloading joblib-1.3.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting regex>=2021.8.3 (from nltk>=3.2.1->newspaper3k)\n",
      "  Downloading regex-2023.10.3-cp312-cp312-win_amd64.whl.metadata (41 kB)\n",
      "     ---------------------------------------- 0.0/42.0 kB ? eta -:--:--\n",
      "     ---------------------------------------- 42.0/42.0 kB ? eta 0:00:00\n",
      "Collecting tqdm (from nltk>=3.2.1->newspaper3k)\n",
      "  Downloading tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)\n",
      "     ---------------------------------------- 0.0/57.6 kB ? eta -:--:--\n",
      "     ---------------------------------------- 57.6/57.6 kB 3.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\2022-pc(t)-41\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.10.0->newspaper3k) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\2022-pc(t)-41\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.10.0->newspaper3k) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\2022-pc(t)-41\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.10.0->newspaper3k) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\2022-pc(t)-41\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.10.0->newspaper3k) (2023.11.17)\n",
      "Collecting requests-file>=1.4 (from tldextract>=2.0.1->newspaper3k)\n",
      "  Downloading requests_file-1.5.1-py2.py3-none-any.whl (3.7 kB)\n",
      "Collecting filelock>=3.0.8 (from tldextract>=2.0.1->newspaper3k)\n",
      "  Downloading filelock-3.13.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\2022-pc(t)-41\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from click->nltk>=3.2.1->newspaper3k) (0.4.6)\n",
      "Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
      "   ---------------------------------------- 0.0/81.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 81.3/81.3 kB ? eta 0:00:00\n",
      "Downloading lxml-4.9.3-cp312-cp312-win_amd64.whl (3.8 MB)\n",
      "   ---------------------------------------- 0.0/3.8 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.3/3.8 MB 17.7 MB/s eta 0:00:01\n",
      "   ----- ---------------------------------- 0.5/3.8 MB 8.6 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 0.9/3.8 MB 7.1 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 1.3/3.8 MB 7.7 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 1.7/3.8 MB 7.7 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 2.1/3.8 MB 7.8 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 2.5/3.8 MB 8.2 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 3.1/3.8 MB 8.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.8/3.8 MB 9.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.8/3.8 MB 8.6 MB/s eta 0:00:00\n",
      "Downloading Pillow-10.1.0-cp312-cp312-win_amd64.whl (2.6 MB)\n",
      "   ---------------------------------------- 0.0/2.6 MB ? eta -:--:--\n",
      "   ----------- ---------------------------- 0.8/2.6 MB 24.1 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 1.2/2.6 MB 15.9 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 2.0/2.6 MB 15.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.6/2.6 MB 15.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.6/2.6 MB 13.9 MB/s eta 0:00:00\n",
      "Downloading tldextract-5.1.1-py3-none-any.whl (97 kB)\n",
      "   ---------------------------------------- 0.0/97.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 97.7/97.7 kB 5.8 MB/s eta 0:00:00\n",
      "Downloading filelock-3.13.1-py3-none-any.whl (11 kB)\n",
      "Downloading regex-2023.10.3-cp312-cp312-win_amd64.whl (268 kB)\n",
      "   ---------------------------------------- 0.0/269.0 kB ? eta -:--:--\n",
      "   --------------------------------------- 269.0/269.0 kB 16.2 MB/s eta 0:00:00\n",
      "Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "   ---------------------------------------- 0.0/97.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 97.9/97.9 kB 5.8 MB/s eta 0:00:00\n",
      "Downloading joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "   ---------------------------------------- 0.0/302.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 302.2/302.2 kB ? eta 0:00:00\n",
      "Downloading tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "   ---------------------------------------- 0.0/78.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 78.3/78.3 kB ? eta 0:00:00\n",
      "Building wheels for collected packages: tinysegmenter, feedfinder2, jieba3k, sgmllib3k\n",
      "  Building wheel for tinysegmenter (pyproject.toml): started\n",
      "  Building wheel for tinysegmenter (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for tinysegmenter: filename=tinysegmenter-0.3-py3-none-any.whl size=13568 sha256=85947ff1e824625421bcdbad898a3fbb894c5459a97a8befadba1306c74fd88f\n",
      "  Stored in directory: c:\\users\\2022-pc(t)-41\\appdata\\local\\pip\\cache\\wheels\\a5\\91\\9f\\00d66475960891a64867914273fcaf78df6cb04d905b104a2a\n",
      "  Building wheel for feedfinder2 (pyproject.toml): started\n",
      "  Building wheel for feedfinder2 (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for feedfinder2: filename=feedfinder2-0.0.4-py3-none-any.whl size=3359 sha256=076ac9006074f772ec0ccb76baf92e4b64dc56d8cb07208d489d47d6e7beafae\n",
      "  Stored in directory: c:\\users\\2022-pc(t)-41\\appdata\\local\\pip\\cache\\wheels\\9f\\9f\\fb\\364871d7426d3cdd4d293dcf7e53d97f160c508b2ccf00cc79\n",
      "  Building wheel for jieba3k (pyproject.toml): started\n",
      "  Building wheel for jieba3k (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for jieba3k: filename=jieba3k-0.35.1-py3-none-any.whl size=7398387 sha256=6c2111d21c8101deba77a71355c9aed443bd09224232031a4115df1378c11391\n",
      "  Stored in directory: c:\\users\\2022-pc(t)-41\\appdata\\local\\pip\\cache\\wheels\\26\\72\\f7\\fff392a8d4ea988dea4ccf9788599d09462a7f5e51e04f8a92\n",
      "  Building wheel for sgmllib3k (pyproject.toml): started\n",
      "  Building wheel for sgmllib3k (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6061 sha256=edaac54ee88b2d6031c56beca403854b28979a53225a225176529503bc3e1304\n",
      "  Stored in directory: c:\\users\\2022-pc(t)-41\\appdata\\local\\pip\\cache\\wheels\\03\\f5\\1a\\23761066dac1d0e8e683e5fdb27e12de53209d05a4a37e6246\n",
      "Successfully built tinysegmenter feedfinder2 jieba3k sgmllib3k\n",
      "Installing collected packages: tinysegmenter, sgmllib3k, jieba3k, tqdm, regex, Pillow, lxml, joblib, filelock, feedparser, cssselect, click, requests-file, nltk, feedfinder2, tldextract, newspaper3k\n",
      "Successfully installed Pillow-10.1.0 click-8.1.7 cssselect-1.2.0 feedfinder2-0.0.4 feedparser-6.0.11 filelock-3.13.1 jieba3k-0.35.1 joblib-1.3.2 lxml-4.9.3 newspaper3k-0.2.8 nltk-3.8.1 regex-2023.10.3 requests-file-1.5.1 sgmllib3k-1.0.0 tinysegmenter-0.3 tldextract-5.1.1 tqdm-4.66.1\n"
     ]
    }
   ],
   "source": [
    "!pip install newspaper3k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "66a057f2-befe-482b-994d-702ab4746ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "㈜엔씨소프트(대표 김택진, 이하 엔씨(NC))가 스마트카 플랫폼 전문기업 ‘오비고’와 차량용 AI 개인 맞춤형 기술 서비스 개발사업을 위한 업무협약(MOU)을 체결했다고 18일 밝혔다.양사는 지난 15일 성남시 분당구에 위치한 오비고 본사에서 MOU 체결식을 진행했다. 이번 업무 협약을 통해 자율주행차 및 커넥티드카 환경에 최적화된 개인 맞춤형 AI 솔루션을 공동 개발하고 서비스 상용화를 위한 마케팅 활동을 추진하기로 결정했다.엔씨(NC)는 차량용 서비스를 위해 자체 개발한 거대 AI 언어모델 ‘VARCO LLM(Large Language Model)’을 제공한다. 엔씨(NC)가 제공하는 AI 기술은 텍스트, 오디오, 이미지 등을 스스로 학습하고 핵심 내용으로 정리하여 운전자에게 맞춤형으로 전달하는 역할을 해줄 예정이다.스마트카 플랫폼 전문기업 오비고는 생성형 AI차량용 서비스 개발과 OEM 양산을 추진한다. 양사는 협업의 최초 결과물인 ‘오비고 브리핑 서비스’를 2024년 세계 최대 IT 박람회 ‘CES’에서 선보일 예정이다. 이후 양사가 보유한 핵심역량과 기술을 활용해 다가오는 AI 인포테인먼트 서비스 분야에서 차별화된 모빌리티 경험을 선사해 나간다는 계획이다.엔씨소프트 이연수 NLP센터장은 “이번 협업은 생성형 AI기술이 콘텐츠, 모빌리티 플랫폼과 만나 운전자에게 새로운 경험을 줄 수 있는 사례”라며 “앞으로도 스마트카 시장에서 엔씨소프트 AI 기술로 다양한 콘텐츠들이 사용자 맞춤형으로 잘 전달될 수 있도록 협력해 나가겠다”고 밝혔다.오비고 황도연 대표는 \"엔씨소프트의 생성형 AI 플랫폼 VARCO를 기반으로 AI기술이 탑재된 다양한 차량용 서비스를 개발하여 운전자 개개인에게 더 나은 가치를 제공해 나가겠다”며 “앞으로 차량에 개인형 맞춤 서비스를 적용하기 위해 다양한 기업과 AI 기술 협업을 적극적으로 추진해 나갈 계획”이라고 강조했다.\n"
     ]
    }
   ],
   "source": [
    "import newspaper\n",
    "\n",
    "url = \"https://www.inven.co.kr/webzine/news/?news=291540\"\n",
    "article = newspaper.Article(url, language='ko')\n",
    "article.download()\n",
    "article.parse()\n",
    "print(article.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e28942cb-d5b9-41fc-ab0b-098339ecfdf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://static.inven.co.kr/column/2023/12/18/news/i8267328605.jpg\n"
     ]
    }
   ],
   "source": [
    "print(article.top_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c90bba9-78f8-4a5c-a709-e00bb74b74e7",
   "metadata": {},
   "source": [
    "### 웹툰 이미지 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "50e2666e-f4b0-4aa5-9598-385a1c092e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./webtoon/001.jpg : OK\n",
      "./webtoon/002.jpg : OK\n",
      "./webtoon/003.jpg : OK\n",
      "./webtoon/004.jpg : OK\n",
      "./webtoon/005.jpg : OK\n",
      "./webtoon/006.jpg : OK\n",
      "./webtoon/007.jpg : OK\n",
      "./webtoon/008.jpg : OK\n",
      "./webtoon/009.jpg : OK\n",
      "./webtoon/010.jpg : OK\n",
      "./webtoon/011.jpg : OK\n",
      "./webtoon/012.jpg : OK\n",
      "./webtoon/013.jpg : OK\n",
      "./webtoon/014.jpg : OK\n",
      "./webtoon/015.jpg : OK\n",
      "./webtoon/016.jpg : OK\n",
      "./webtoon/017.jpg : OK\n",
      "./webtoon/018.jpg : OK\n",
      "./webtoon/019.jpg : OK\n",
      "./webtoon/020.jpg : OK\n",
      "./webtoon/021.jpg : OK\n",
      "./webtoon/022.jpg : OK\n",
      "./webtoon/023.jpg : OK\n",
      "./webtoon/024.jpg : OK\n",
      "./webtoon/025.jpg : OK\n",
      "./webtoon/026.jpg : OK\n",
      "./webtoon/027.jpg : OK\n",
      "./webtoon/028.jpg : OK\n",
      "./webtoon/029.jpg : OK\n",
      "./webtoon/030.jpg : OK\n",
      "./webtoon/031.jpg : OK\n",
      "./webtoon/032.jpg : OK\n",
      "./webtoon/033.jpg : OK\n",
      "./webtoon/034.jpg : OK\n",
      "./webtoon/035.jpg : OK\n",
      "./webtoon/036.jpg : OK\n",
      "./webtoon/037.jpg : OK\n",
      "./webtoon/038.jpg : OK\n",
      "./webtoon/039.jpg : OK\n",
      "./webtoon/040.jpg : OK\n",
      "./webtoon/041.jpg : OK\n",
      "./webtoon/042.jpg : OK\n",
      "./webtoon/043.jpg : OK\n",
      "./webtoon/044.jpg : OK\n",
      "./webtoon/045.jpg : OK\n",
      "./webtoon/046.jpg : OK\n",
      "./webtoon/047.jpg : OK\n",
      "./webtoon/048.jpg : OK\n",
      "./webtoon/049.jpg : OK\n",
      "./webtoon/050.jpg : OK\n",
      "./webtoon/051.jpg : OK\n",
      "./webtoon/052.jpg : OK\n",
      "./webtoon/053.jpg : OK\n",
      "./webtoon/054.jpg : OK\n",
      "./webtoon/055.jpg : OK\n",
      "./webtoon/056.jpg : OK\n",
      "./webtoon/057.jpg : OK\n",
      "./webtoon/058.jpg : OK\n",
      "./webtoon/059.jpg : OK\n",
      "./webtoon/060.jpg : OK\n",
      "./webtoon/061.jpg : OK\n",
      "./webtoon/062.jpg : OK\n",
      "./webtoon/063.jpg : OK\n",
      "./webtoon/064.jpg : OK\n",
      "./webtoon/065.jpg : OK\n",
      "./webtoon/066.jpg : OK\n",
      "./webtoon/067.jpg : OK\n",
      "./webtoon/068.jpg : OK\n",
      "./webtoon/069.jpg : OK\n",
      "./webtoon/070.jpg : OK\n",
      "./webtoon/071.jpg : OK\n",
      "./webtoon/072.jpg : OK\n",
      "./webtoon/073.jpg : OK\n",
      "./webtoon/074.jpg : OK\n",
      "./webtoon/075.jpg : OK\n",
      "./webtoon/076.jpg : OK\n",
      "./webtoon/077.jpg : OK\n",
      "./webtoon/078.jpg : OK\n",
      "./webtoon/079.jpg : OK\n",
      "./webtoon/080.jpg : OK\n",
      "./webtoon/081.jpg : OK\n",
      "./webtoon/082.jpg : OK\n",
      "./webtoon/083.jpg : OK\n",
      "./webtoon/084.jpg : OK\n",
      "./webtoon/085.jpg : OK\n",
      "./webtoon/086.jpg : OK\n",
      "./webtoon/087.jpg : OK\n",
      "./webtoon/088.jpg : OK\n",
      "./webtoon/089.jpg : OK\n",
      "./webtoon/090.jpg : OK\n",
      "./webtoon/091.jpg : OK\n",
      "./webtoon/092.jpg : OK\n",
      "./webtoon/093.jpg : OK\n",
      "./webtoon/094.jpg : OK\n",
      "./webtoon/095.jpg : OK\n",
      "./webtoon/096.jpg : OK\n",
      "./webtoon/097.jpg : OK\n",
      "./webtoon/098.jpg : OK\n",
      "./webtoon/099.jpg : OK\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://comic.naver.com/webtoon/detail.nhn?titleId=747961&no=2\"\n",
    "html = requests.get(url).text\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "if not(os.path.isdir(\"./webtoon\")):\n",
    "    os.makedirs(os.path.join(\"./webtoon\"))\n",
    "\n",
    "#파일 1부터 만들\n",
    "i = 1\n",
    "for tag in soup.select('.wt_viewer img'):\n",
    "    img_url = tag['src']\n",
    "    save_img = \"./webtoon/\" + str(i).zfill(3) + img_url[-4:]\n",
    "    i += 1\n",
    "    print(save_img + \" : OK\")\n",
    "    headers = {'Referer': img_url}\n",
    "    img_data = requests.get(img_url, headers=headers).content\n",
    "    \n",
    "    with open(save_img, 'wb') as f:\n",
    "        f.write(img_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18949d6c-33c0-41c9-802c-3278cbb4b307",
   "metadata": {},
   "source": [
    "### selenium 데이터 수집"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f7e9584c-0557-4e6c-884f-e1012af3e452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting selenium\n",
      "  Downloading selenium-4.16.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in c:\\users\\2022-pc(t)-41\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (2.1.0)\n",
      "Collecting trio~=0.17 (from selenium)\n",
      "  Downloading trio-0.23.2-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting trio-websocket~=0.9 (from selenium)\n",
      "  Downloading trio_websocket-0.11.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\2022-pc(t)-41\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from selenium) (2023.11.17)\n",
      "Requirement already satisfied: attrs>=20.1.0 in c:\\users\\2022-pc(t)-41\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from trio~=0.17->selenium) (23.1.0)\n",
      "Collecting sortedcontainers (from trio~=0.17->selenium)\n",
      "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
      "Requirement already satisfied: idna in c:\\users\\2022-pc(t)-41\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from trio~=0.17->selenium) (3.6)\n",
      "Collecting outcome (from trio~=0.17->selenium)\n",
      "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\2022-pc(t)-41\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\2022-pc(t)-41\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from trio~=0.17->selenium) (1.16.0)\n",
      "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
      "  Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Collecting pysocks!=1.5.7,<2.0,>=1.5.6 (from urllib3[socks]<3,>=1.26->selenium)\n",
      "  Downloading PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: pycparser in c:\\users\\2022-pc(t)-41\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Collecting h11<1,>=0.9.0 (from wsproto>=0.14->trio-websocket~=0.9->selenium)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "     ---------------------------------------- 0.0/58.3 kB ? eta -:--:--\n",
      "     ---------------------------------------- 58.3/58.3 kB 3.0 MB/s eta 0:00:00\n",
      "Downloading selenium-4.16.0-py3-none-any.whl (10.0 MB)\n",
      "   ---------------------------------------- 0.0/10.0 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.4/10.0 MB 23.7 MB/s eta 0:00:01\n",
      "   ---- ----------------------------------- 1.0/10.0 MB 12.9 MB/s eta 0:00:01\n",
      "   ------- -------------------------------- 1.9/10.0 MB 15.1 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 2.7/10.0 MB 14.3 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 3.3/10.0 MB 15.1 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 3.8/10.0 MB 14.4 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 4.4/10.0 MB 14.0 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 5.0/10.0 MB 13.9 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 5.6/10.0 MB 13.8 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 6.8/10.0 MB 15.0 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 7.4/10.0 MB 14.7 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 7.7/10.0 MB 14.8 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 8.8/10.0 MB 14.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 9.5/10.0 MB 14.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.0/10.0 MB 14.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.0/10.0 MB 14.2 MB/s eta 0:00:00\n",
      "Downloading trio-0.23.2-py3-none-any.whl (461 kB)\n",
      "   ---------------------------------------- 0.0/461.6 kB ? eta -:--:--\n",
      "   --------------------------------------- 461.6/461.6 kB 14.1 MB/s eta 0:00:00\n",
      "Downloading trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n",
      "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
      "Installing collected packages: sortedcontainers, pysocks, outcome, h11, wsproto, trio, trio-websocket, selenium\n",
      "Successfully installed h11-0.14.0 outcome-1.3.0.post0 pysocks-1.7.1 selenium-4.16.0 sortedcontainers-2.4.0 trio-0.23.2 trio-websocket-0.11.1 wsproto-1.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2a994680-f5d4-421d-843a-08e48f526032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting webdriver_manager\n",
      "  Downloading webdriver_manager-4.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\2022-pc(t)-41\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from webdriver_manager) (2.31.0)\n",
      "Collecting python-dotenv (from webdriver_manager)\n",
      "  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\2022-pc(t)-41\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from webdriver_manager) (23.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\2022-pc(t)-41\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->webdriver_manager) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\2022-pc(t)-41\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->webdriver_manager) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\2022-pc(t)-41\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->webdriver_manager) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\2022-pc(t)-41\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->webdriver_manager) (2023.11.17)\n",
      "Downloading webdriver_manager-4.0.1-py2.py3-none-any.whl (27 kB)\n",
      "Installing collected packages: python-dotenv, webdriver_manager\n",
      "Successfully installed python-dotenv-1.0.0 webdriver_manager-4.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install webdriver_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "39c4ba9f-adbb-4f12-af70-63ac65126404",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "driver.implicitly_wait(3)\n",
    "driver.get('https://naver.com')\n",
    "time.sleep(1)\n",
    "\n",
    "elem1 = driver.find_element(By.ID, \"query\")\n",
    "elem1.send_keys(\"강아지\")\n",
    "time.sleep(1)\n",
    "\n",
    "elem2 = driver.find_element(By.ID, \"search-btn\")\n",
    "elem2.click()\n",
    "time.sleep(1)\n",
    "\n",
    "elem3 = driver.find_element(By.XPATH, '//*[@id=\"lnb\"]/div[1]/div/div[1]/div/div[1]/div[1]/a')\n",
    "elem3.click()\n",
    "\n",
    "elem4 = driver.find_element(By.XPATH, '//*[@id=\"main_pack\"]/section[2]/div[1]/div/div/div[1]/div[1]/div/div/div/img')\n",
    "elem4.click()\n",
    "\n",
    "elem5 = driver.find_element(By.XPATH, '//*[@id=\"main_pack\"]/section[1]/div/div/div[1]/div[2]/div[1]/img')\n",
    "imgurl = elem5.get_attribute('src')\n",
    "\n",
    "headers = {'Referer': imgurl}\n",
    "img_data = requests.get(imgurl, headers=headers).content\n",
    "save_img = \"dog.png\"\n",
    "with open(save_img, 'wb') as f:\n",
    "    f.write(img_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ade60887-366e-4466-8572-38b289c918ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import urllib.request\n",
    "import time\n",
    "\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "driver.implicitly_wait(3)\n",
    "driver.get(\"https://www.google.com/search?q=%EA%B0%95%EC%95%84%EC%A7%80&sca_esv=591793731&hl=ko&tbm=isch&sxsrf=AM9HkKnr6GxtmPATaKGhG_Fttu6WzGCKBw:1702886716271&source=lnms&sa=X&ved=2ahUKEwiKnsOuw5iDAxX7k1YBHeGAAuYQ_AUoAnoECAMQBA&biw=1280&bih=620&dpr=1.25\")\n",
    "time.sleep(1)\n",
    "\n",
    "try:\n",
    "    elem1 = driver.find_element(By.XPATH, '//*[@id=\"islrg\"]/div[1]/div[1]/a[1]/div[1]/img')\n",
    "    imgurl = elem1.get_attribute('src')\n",
    "    opener = urllib.request.build_opener()\n",
    "    opener.addheaders = [\n",
    "        ('User-Agent', 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1941.0 Safari/537.36')\n",
    "    ]\n",
    "    urllib.request.install_opener(opener)\n",
    "    urllib.request.urlretrieve(imgurl, f'./dog.jpg')\n",
    "except Exception as e:\n",
    "    print('e : ', e)\n",
    "    pass\n",
    "\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba3b401-82e1-4de1-ba70-a60c219dc8b2",
   "metadata": {},
   "source": [
    "### 과제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f99e0ae-5f0d-4271-9dc5-693e5b688cfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./news/001.jpg : OK\n",
      "./news/002.jpg : OK\n",
      "./news/003.jpg : OK\n",
      "./news/004.jpg : OK\n",
      "./news/005.jpg : OK\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import urllib.request\n",
    "import time\n",
    "import requests\n",
    "\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "driver.implicitly_wait(3)\n",
    "driver.get(\"https://www.skttechacademy.com/\")\n",
    "time.sleep(1)\n",
    "\n",
    "elem1 = driver.find_element(By.XPATH, \"/html/body/main/section[1]/div/div[2]/a\")\n",
    "elem1.click()\n",
    "time.sleep(1)\n",
    "\n",
    "try:\n",
    "    i=1\n",
    "    for i in range(1,6):\n",
    "        time.sleep(1)\n",
    "        x_path = f'/html/body/main/div[4]/ul/li[{i}]/figure/span/img'\n",
    "        elem2 = driver.find_element(By.XPATH, x_path)\n",
    "        img_url = elem2.get_attribute('src')\n",
    "        save_img = \"./news/\" + str(i).zfill(3) + \".jpg\"\n",
    "        print(save_img + \" : OK\")\n",
    "        headers = {'Referer': img_url}\n",
    "        img_data = requests.get(img_url, headers=headers).content\n",
    "    \n",
    "        with open(save_img, 'wb') as f:\n",
    "            f.write(img_data)\n",
    "except Exception as e:\n",
    "    print('e : ', e)\n",
    "    pass\n",
    "\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2fa10db6-e864-4172-99cb-4ce808376aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./news/001.jpg : OK\n",
      "./news/002.jpg : OK\n",
      "./news/003.jpg : OK\n",
      "./news/004.jpg : OK\n",
      "./news/005.jpg : OK\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import requests\n",
    "import time\n",
    "import os\n",
    "\n",
    "# 웹드라이버 설정 및 초기화\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "driver.implicitly_wait(3)\n",
    "\n",
    "# 웹 사이트 열기\n",
    "driver.get(\"https://www.skttechacademy.com/\")\n",
    "time.sleep(1)\n",
    "\n",
    "# 클래스 이름을 사용하여 요소 찾기 (SKT TECH LAB)\n",
    "more_button = driver.find_element(By.CLASS_NAME, \"btn_more\")\n",
    "more_button.click()\n",
    "time.sleep(1)\n",
    "\n",
    "# 이미지 저장 폴더 확인 및 생성\n",
    "save_dir = \"./news/\"\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "# 이미지를 다운로드하는 과정\n",
    "try:\n",
    "    i=1\n",
    "    for i in range(1,6):\n",
    "        time.sleep(1)\n",
    "        # 각 이미지에 대한 XPath를 생성하고 요소를 찾음\n",
    "        x_path = f'/html/body/main/div[4]/ul/li[{i}]/figure/span/img'\n",
    "        elem = driver.find_element(By.XPATH, x_path)\n",
    "        img_url = elem.get_attribute('src')\n",
    "        save_img = os.path.join(save_dir, f\"{str(i).zfill(3)}.jpg\")\n",
    "        print(save_img + \" : OK\")\n",
    "\n",
    "        # 이미지 URL에서 이미지를 다운로드하고 저장\n",
    "        headers = {'Referer': img_url}\n",
    "        img_data = requests.get(img_url, headers=headers).content\n",
    "        with open(save_img, 'wb') as f:\n",
    "            f.write(img_data)\n",
    "            \n",
    "except Exception as e:\n",
    "    print('e : ', e)\n",
    "    pass\n",
    "\n",
    "# 드라이버 종료\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8adcef-70b5-4e52-bf0f-1416a28e94f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
